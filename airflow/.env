# Custom
# COMPOSE_PROJECT_NAME=airflow
GOOGLE_APPLICATION_CREDENTIALS=/.google/credentials/google_credentials.json
AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json

AIRFLOW_UID=50000

PG_HOST=pgdatabase
PG_USER=root
PG_PASSWORD=root
PG_PORT=5432
PG_DATABASE=ny_taxi

# Google Cloud Resource Parameters
# GCP_PROJECT_ID=de-nyc-taxi-project
# GCP_GCS_BUCKET=de_nyc_taxi_project_data-lake
# BIGQUERY_DATASET=de_nyc_taxi_project_all_data

# Postgres
# POSTGRES_USER=airflow
# POSTGRES_PASSWORD=airflow
# POSTGRES_DB=airflow

# Airflow
# AIRFLOW__CORE__EXECUTOR=LocalExecutor
# AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=10

# AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
# AIRFLOW_CONN_METADATA_DB=postgres+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/airflow
# AIRFLOW_VAR__METADATA_DB_SCHEMA=airflow

# _AIRFLOW_WWW_USER_CREATE=True
# _AIRFLOW_WWW_USER_USERNAME=${_AIRFLOW_WWW_USER_USERNAME:-airflow}
# _AIRFLOW_WWW_USER_PASSWORD=${_AIRFLOW_WWW_USER_PASSWORD:-airflow}

# AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
# AIRFLOW__CORE__LOAD_EXAMPLES=False

# AIRFLOW__CORE__DONOT_PICKLE=False (tried for udf pickling problem)

# _PIP_ADDITIONAL_REQUIREMENTS=${_PIP_ADDITIONAL_REQUIREMENTS:-pyarrow==5.0.0 apache-airflow==2.2.3 apache-airflow-providers-google==6.2.0 pyspark==3.2.1}